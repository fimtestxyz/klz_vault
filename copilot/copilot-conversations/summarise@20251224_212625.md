---
epoch: 1766582785660
modelKey: "qwen/qwen2.5-vl-7b|lm-studio"



tags:
  - copilot-conversation
---

**user**: summarise 
[Context: Notes: 00-Inbox/YT - Why everyone HATES GPT-5 (and how to fix it).md]
[Timestamp: 2025/12/24 21:26:25]

**ai**: 
<errorChunk>Connection error.
more message: Request failed, status 404. {"error":{"message":"No models loaded. Please load a model in the developer page or use the `lms load` command.","type":"invalid_request_error","param":"model","code":"model_not_found"}}</errorChunk>
[Timestamp: 2025/12/24 21:26:35]

**user**: summarise 
[Context: Notes: 00-Inbox/YT - Why everyone HATES GPT-5 (and how to fix it).md]
[Timestamp: 2025/12/24 21:28:26]

**ai**: 
<errorChunk>Trying to keep the first 35872 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input</errorChunk>
[Timestamp: 2025/12/24 21:28:26]

**user**: summarise
[Context: Notes: 00-Inbox/YT - Why everyone HATES GPT-5 (and how to fix it).md]
[Timestamp: 2025/12/24 21:29:03]

**ai**: 
<errorChunk>Trying to keep the first 35872 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input</errorChunk>
[Timestamp: 2025/12/24 21:29:03]